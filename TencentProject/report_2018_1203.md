## 腾讯项目组汇报（二十）--2018/11/30


<h1>Tensorflow Serving部署与调用总结</h1>

<h2>一、Tensorflow Serving 安装</h2>

<h3>安装Tensorflow Serving ModelServer</h3>

(可以看作模型发布器，详细内容参阅[Tensorflow Serving 架构](https://www.tensorflow.org/serving/overview?hl=zh-cn))

三种安装方式：

1. 使用Docker安装（多平台支持，最简单安装方式，最简单的支持GPU发布模型）；
2. 使用APT安装（针对Ubuntu linux，对于其他版本linux，请使用从源码安装或者Docker安装）；
3. 从源码安装（可定制，易出错）；

<h3>1. 使用Docker安装</h3>

只需安装docker，并将docker中Tensorflow Serving的镜像拉取pull下来即可；

不熟悉docker的请参阅[Using TensorFlow Serving with Docker](https://www.tensorflow.org/serving/docker?hl=zh-cn)或者[Docker菜鸟教程](http://www.runoob.com/docker/docker-command-manual.html);

<h3>2. 使用APT安装</h3>

Tensorflow Serving ModelServer binary存在两种版本：

+ **tensorflow-model-server**: Fully optimized server that uses some platform specific compiler optimizations like SSE4 and AVX instructions. This should be the preferred option for most users, but may not work on some older machines.

+ **tensorflow-model-server-universal**: Compiled with basic optimizations, but doesn't include platform specific instruction sets, so should work on most if not all machines out there. Use this if tensorflow-model-server does not work for you. Note that the binary name is the same for both packages, so if you already installed tensorflow-model-server, you should first uninstall it using
    
    ```
    apt-get remove tensorflow-model-server
    ```

> Note: In the above commands, replace tensorflow-model-server with tensorflow-model-server-universal if your processor does not support AVX instructions.


**安装**：

1. 添加TensorFlow Serving distribution URI作为安装包源package source (一次性设置);

    ```
    echo "deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \
curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -
    ```
    
2. 安装并更新Tensorflow ModelServer;

    ```
    apt-get update && apt-get install tensorflow-model-server
    ```

安装完成后则可使用二进制命令`tensorflow_model_server`来发布模型；

**更新**：

    ```
    apt-get upgrade tensorflow-model-server
    ```

<h3>3. 从源码安装</h3>

使用源码安装也要借用到docker，google将各种依赖打包上传到了docker的一个镜像中。

**安装Docker，获得依赖**

docker官方安装指南：<https://docs.docker.com/install/>

**将构建脚本/源代码git clone到本地(clone the build script)**

安装Docker之后，我们需要获取我们想要构建的源代码。我们将使用Git clone TensorFlow serving的主分支：

```
git clone https://github.com/tensorflow/serving.git
cd serving  # 进入目录很重要
```

**构建**

为了建立一个所有依赖关系的密封环境，我们将使用该run_in_docker.sh脚本。此脚本将构建命令传递给Docker容器。默认情况下，脚本将使用latest nightly Docker development image构建。

TensorFlow Serving使用Bazel作为其构建工具。您可以使用Bazel命令构建单个目标或整个源树。

要构建整个树，请执行(最后先看下述的[构建特定版本的Tensorflow serving]()，这样构建的就不是默认的Latest Nightly version)：

```
tools/run_in_docker.sh bazel build -c opt tensorflow_serving/...
```

二进制文件放在bazel-bin目录中，可以使用如下命令运行：(这就是源码安装好的tensorflow serving指令，调用方式如下)

```
bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server
```

要测试您构建的tensorflow serving，请执行：

```
tools/run_in_docker.sh bazel test -c opt tensorflow_serving/...
```

**构建特定版本的Tensorflow Serving**

如果要从特定分支（例如发布分支）构建，请传递`-b <branchname>`给`git clone`命令。

我们还希望通过给`run_in_docker.sh`脚本传递我们想要使用的Docker开发映像，来匹配该代码分支的构建环境。

例如，要构建TensorFlow服务版本1.10：

```
$ git clone -b r1.10 https://github.com/tensorflow/serving.git
...
$ cd serving
$ tools/run_in_docker.sh -d tensorflow/serving:1.10-devel \
  bazel build tensorflow_serving/...
...
```

**Optimized build**

If you'd like to apply generally recommended optimizations, including utilizing platform-specific instruction sets for your processor, you can add `--config=nativeopt` to Bazel build commands when building TensorFlow Serving.

For example:

```
tools/run_in_docker.sh bazel build --config=nativeopt tensorflow_serving/...
```

It's also possible to compile using specific instruction sets (e.g. AVX). Wherever you see bazel build in the documentation, simply add the corresponding flag:


| Instruction Set | Flags |
| --- | --- |
| AVX | --copt=mavx |
| AVX2 | --copt=mavx2 |
| FMA | --copt=mfma |
| SSE 4.1 | --copt=msse4.1 |
| SSE 4.2 | --copt=msse4.2 |
| All supported by processor | --copt=-march=native |

> Note: These instruction sets are not available on all machines, especially with older processors. Use --copt=-march=native if you're unsure what you need.

**Building with GPU Support**

In order to build a custom version of TensorFlow Serving with GPU support, we recommend either building with the [provided Docker images](https://www.tensorflow.org/serving/docker?hl=zh-cn#developing-with-docker), or following the approach in the [GPU Dockerfile](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/tools/docker/Dockerfile.devel-gpu).


<h2>二、Tensorflow Serving 模型部署</h2>

<h3>1. 导出模型</h3>

使用Tensorflow底层API的SavedModel来导出并保存模型；

详情请参阅[使用Tensorflow Core保存和恢复模型](https://github.com/charosen/ClassNotes/blob/master/NeuralNetwork/tensorflow_learnings/2_5_tf_lowapi_save_restore.ipynb)；

<h3>2. 使用Tensorflow Serving ModelServer发布模型</h3>

**使用`Docker+Tensorflow Serving`发布模型**

方法一：

创建并执行一个容器，将主机端口映射到容器端口，并在该容器中发布模型；

要使用Docker发布模型，您需要：

+ 可供Tensorflow serving发布模型的主机上的开放端口；
+ 要发布的SavedModel
+ 客户端将用来请求的模型名称

你要做的是运行Docker容器（`docker run`）， 将容器的端口发布到主机的端口(`-p 主机端口:容器端口`参数)，并将主机的SavedModel路径挂载到容器发布模型的路径。

我们来看一个例子：

```
docker run -p 8501:8501 \
  --mount type=bind,source=/path/to/my_model/,target=/models/my_model \
  -e MODEL_NAME=my_model -t tensorflow/serving
```

在这种情况下，我们启动了一个Docker容器，发布REST API端口8501到我们主机的端口8501，获取名为`my_model`的模型并将其绑定到默认模型基路径（`${MODEL_BASE_PATH}/${MODEL_NAME}`= `/models/my_model`）。最后，我们填补了环境变量 `MODEL_NAME`为`my_model`，填补`MODEL_BASE_PATH`为它的默认值。


也可以使用`--model_config_file`参数来发布多个模型：

```
docker run -p 8500:8500 8501:8501 \
  --mount type=bind,source=/path/to/my_model/,target=/models/my_model \
  --mount type=bind,source=/path/to/my/models.config,target=/models/models.config \
  -t tensorflow/serving --model_config_file=/models/models.config
```

(上述方法，每调用`docker run`命令就会创建新的容器，为在同一个容器中发布多个模型，请使用方法二)

方法二：

使用`docker cp`命令将本机模型复制到指定容器中，创建并执行容器（`docker run`），进入容器（`docker attach`）中使用`tensorflow_model_server`指令来发布模型；


**使用`Tensorflow Serving`发布模型**

指令如下：

```
tensorflow_model_server --port=8500 --rest_api_port=8501 \
  --model_name=my_model --model_base_path=/models/my_model
```

也可使用`--model_config_file`参数来同时发布多个模型，配置文件格式：

```
model_config_list: {
  config: {
    name: "mnist",
    base_path: "/tmp/mnist_model",
    model_platform: "tensorflow"
  },
  config: {
    name: "inception",
    base_path: "/tmp/inception_model",
    model_platform: "tensorflow"
  }
}
```
发布时，命令就变成：

```
tensorflow_model_server --port=9000 --model_config_file=配置文件
```

但是在多模型时，[TF Serving无法动态刷新配置文件](https://github.com/tensorflow/serving/issues/380)(也就是当在配置文件中添加了新的模型，tfserving不会自动发布，而是需要重新调用一次`tensorflow_model_server`来使新的配置文件生效)；

<h3>Tensorflow Serving 高级发布方法--构建自己的Tensorflow ModelServer</h3>

上述教程都是使用的安装好的Tensorflow ModelServer，因此存在无法动态发现和发布新版本的训练好的模型，这时就需要自定义ModelServer以支持动态发现和发布模型；

（这部分内容设计c++编程，附上链接[Building Standard TensorFlow ModelServer](https://www.tensorflow.org/serving/serving_advanced?hl=zh-cn)）


<h2>三、Tensorflow Serving 调用模型服务</h2>

<h3>1. 通过RESTful API调用</h3>

Tensorflow serving的RESTful API包括：模型状态API，模型元数据API， 分类与回归API，预测API；

详情参阅[Tensorflow serving RESTful API](https://www.tensorflow.org/serving/api_rest?hl=zh-cn);


<h3>2. 通过gRPC API调用</h3>

**Tensorflow Serving Python API客户端包安装**

```
pip install tensorflow-serving-api
```

**编写gRPC客户端**：

gRPC基础：

1. <https://www.jianshu.com/p/b3cd8e20aa2c>;
2. <https://grpc.io/docs/quickstart/python.html>;

官方gRPC tensorflow serving客户端示例代码：(用到了多线程condition)

<https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/mnist_client.py>;

网上找到的一个不用多线程的gRPC tensorflow serving客户端示例代码：（强力推荐）

<https://x-algo.cn/index.php/2017/05/02/2221/>;

还有一个神教程，上面提及到tensorflow serving不能动态加载更新模型，以下博客写了一个动态加载模型的代码：

<https://blog.csdn.net/hahajinbu/article/details/81945149>;

还有tensorflow-serving-api及tensorflow源码阅读
